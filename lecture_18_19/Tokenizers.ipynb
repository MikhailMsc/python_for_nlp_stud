{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad1bded-53f7-45e1-ae46-830e39fee916",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "–ö–∞–∫ –≤—ã –ø–æ–º–Ω–∏—Ç–µ, —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —ç—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å –∫–æ–≥–¥–∞ –º—ã —Ä–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ –∏–ª–∏ —á–∞—Å—Ç–∏ —Å–ª–æ–≤. –ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏–º–µ–µ—Ç –≤–Ω—É—Ç—Ä–∏ —Å–µ–±—è –∫–∞–∫—É—é —Ç–æ —Ç–∞–±–ª–∏—Ü—É, –≥–¥–µ –Ω–∞ –ø—Ä–æ—Ç–∏–≤ id-—à–Ω–∏–∫–∞ —Å—Ç–æ–∏—Ç —Ç–æ–∫–µ–Ω (—Å–ª–æ–≤–æ –∏–ª–∏ —á–∞—Å—Ç—å). –ò –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –æ–Ω —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞ –∏–∑ —Å–≤–æ–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è –∏ –∑–∞—Ç–µ–º –º–µ–Ω—è–µ—Ç —Å–ª–æ–≤–æ –Ω–∞ –µ–≥–æ id. –í –∏—Ç–æ–≥–µ –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º—ã –ø–æ–ª—É—á–∞–µ–º –Ω–µ —Ç–µ–∫—Å—Ç –∞ —Å–ø–∏—Å–æ–∫ id-—à–Ω–∏–∫–æ–≤.\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å —Ç–æ–ª—å–∫–æ –∫–∞–∫ —Å–æ—Å—Ç–∞–≤–∏—Ç—å —ç—Ç—É —Ç–∞–±–ª–∏—Ü—É-—Å–ª–æ–≤–∞—Ä—å –∏ –∫–∞–∫ —Ä–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ. –¢–µ–º –±–æ–ª–µ–µ —á—Ç–æ –Ω–µ –≤–æ –≤—Å–µ—Ö —è–∑—ã–∫–∞—Ö –µ—Å—Ç—å –ø–æ–Ω—è—Ç–∏–µ –ø—Ä–æ–±–µ–ª–∞, –¥–∞ —Ä–∞–∑–±–∏–≤–∞—Ç—å –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –Ω–µ —Å–∞–º—ã–π —Ç–æ—á–Ω—ã–π —Å–ø–æ—Å–æ–±.\n",
    "\n",
    "–ú—ã —É–∂–µ –∑–Ω–∞–∫–æ–º—ã —Å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º –æ—Ç SpaCy –∏ –æ–Ω –±—ã–ª rule-based —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä, —Ç–æ –µ—Å—Ç—å –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞—Ö. –¢–∞–∫ –∂–µ –º—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ –ø—Ä–æ–±–µ–ª–∞—Ö –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏.\n",
    "\n",
    "–ö–∞–∫–∏–µ –º–∏–Ω—É—Å—ã –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏ —Ä–∞–∑–±–∏–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏:  \n",
    "- —Å–ª–æ–≤–∞—Ä—å –º–æ–∂–µ—Ç —Å—Ç–∞—Ç—å –æ—á–µ–Ω—å –±–æ–ª—å—à–∏–º, —Ç–∞–∫ –∫–∞–∫ —Å–ª–æ–≤ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ  \n",
    "- —Å–ª–æ–∂–Ω–æ –Ω–∞–ø–∏—Å–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è, –≥–¥–µ-—Ç–æ —Ç–æ—á–∫–∞ –∑–Ω–∞–∫ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –≥–¥–µ —á–∞—Å—Ç—å —Å–º–∞–π–ª–∞, –≥–¥–µ-—Ç–æ —á–∞—Å—Ç—å —Ç—Ä–æ–µ—Ç–æ—á–∏—è –∏ —Ç–∞–∫–∏—Ö –Ω—é–∞–Ω—Å–æ–≤ —Ç—ã—Å—è—á–∏\n",
    "    \n",
    "    \n",
    "–°–µ–≥–æ–¥–Ω—è —É–≥–ª—É–±–∏–º—Å—è –≤ –ª–æ–≥–∏–∫—É –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤, –ø–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ —Ç–∏–ø—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –±—ã–≤–∞—é—Ç –∏ –ø–æ–π–º–µ–º –∫–∞–∫ –∏ –≥–¥–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≥–æ—Ç–æ–≤—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7431261-57c1-404d-973c-3111225c681f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word tokenization\n",
    "–ï—Å–ª–∏ –≤–∑—è—Ç—å –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤, —Ç–æ –≤—ã –ø–æ–ª—É—á–∏—Ç–µ –æ–≥—Ä–æ–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤. –ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –≤—Å–µ –∏—Ö –∑–∞–ø–æ–º–Ω–∏—Ç—å –∏ –æ–±—Ä–∞–±–∞—Ç–≤—ã–≤–∞—Ç—å, —Ç–æ –≤–∞–º –ø—Ä–∏–¥–µ—Ç—Å—è –¥–µ—Ä–∂–∞—Ç—å –æ–≥—Ä–æ–º–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –≤–Ω—É—Ç—Ä–∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞, —á—Ç–æ —Å–∫–∞–∂–µ—Ç—Å—è –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏/–ø–∞–º—è—Ç–∏. –ù–æ –¥–∞–∂–µ –Ω–µ —ç—Ç–æ —Å—Ç—Ä–∞—à–Ω–æ, –≤—ã –±—É–¥–µ—Ç–µ –∫–æ–ø–∏—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ –≤—Å–µ –æ–ø–µ—á–∞—Ç–∫–∏, –∞ —Ç–∞–∫ –∂–µ —Å–ª–æ–≤–∞ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å —Ä–∞–∑ –∏–ª–∏ –¥–≤–∞. –ü–æ —Ç–∞–∫–∏–º —Ä–µ–¥–∫–∏–º —Å–ª–æ–≤–∞–º —É –≤–∞—Å –Ω–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏ –ª—é–±–∞—è –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –ø–æ–¥—Å—Ç—Ä–∞–∏–≤–∞—Ç—å—Å—è –ø–æ–¥ –Ω–∏—Ö –±—É–¥–µ—Ç —É—á–∏—Ç—å —è–≤–Ω–æ –Ω–µ —Ç–æ —á—Ç–æ –≤—ã —Ö–æ—Ç–∏—Ç–µ –æ—Ç –Ω–µ–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90dcf8-57dd-4fa2-a30b-3dbae78e1a26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Character tokenization\n",
    "–ü—Ä–æ–±–ª–µ–º—É –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –ø–µ—Ä–µ–π–¥—è –Ω–∞ —Å–∏–º–≤–æ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä. –ù–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã –ø–æ—è–≤–ª—è—é—Ç—Å—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —Ä–µ–∂–µ —á–µ–º —Å–ª–æ–≤–∞.  \n",
    "–ù–æ —ç—Ç–æ—Ç –ø–æ–¥—Ö–æ–¥ —Ä–æ–∂–¥–∞–µ—Ç —Å–≤–æ–∏ –ø—Ä–æ–±–ª–µ–º—ã:  \n",
    "- —Ç–µ–∫—Å—Ç —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –æ—á–µ–Ω—å –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é, –º–æ–¥–µ–ª—è–º —É–∂–µ –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ —É–ª–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ —ç—Ç–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏  \n",
    "- —ç–º–±–µ–¥–∏–Ω–≥–∏ —Å–∏–º–≤–æ–ª–æ–≤ –Ω–µ—Å—É—Ç –æ—á–µ–Ω—å –º–∞–ª–æ –ø–æ–ª–µ–∑–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –Ω–∞—Å, –≤ –æ—Ç–ª–∏—á–∏–∏ –æ—Ç —ç–º–±–µ–¥–∏–Ω–≥–æ–≤ —Å–ª–æ–≤. \n",
    "\n",
    "**–¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä—ã –Ω–µ —Å–æ–¥–µ—Ä–∞–∂–∞—Ç —ç–º–±–µ–¥–∏–Ω–≥–∏! –û–Ω–∏ –Ω—É–∂–Ω—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758a1b0-3db9-4b3f-84c1-a14e96f531d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Subword tokenization\n",
    "–î–∞–Ω–Ω—ã–π –≤–∏–¥ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–≤ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã, –≥–¥–µ —Ç–æ–∫–µ–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–æ–º –∏–ª–∏ —á–∞—Å—Ç—å—é —Å–ª–æ–≤–∞.   \n",
    "–ü—Ä–∏–Ω—Ü–∏–ø —Å–ª–µ–¥—É—é—â–∏–π - –ï—Å–ª–∏ —Å–ª–æ–≤–æ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è, —Ç–æ –º—ã –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –µ–≥–æ —Ü–µ–ª–∏–∫–æ–≤ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω. –ï—Å–ª–∏ —Ä–µ–¥–∫–æ, —Ç–æ —Å–º–æ—Ç—Ä–∏–º –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ —Å–ª–æ–≤–∞ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ –Ω–∞ –Ω–∏—Ö.  \n",
    "–ü–æ–ª—É—á–∞–µ—Ç—Å—è —á—Ç–æ —Ä–µ—à–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã:\n",
    "- —É—à–ª–∏ –æ—Ç –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–≥–æ —Ä–∞–∑—Ä–∞—Å—Ç–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è\n",
    "- –Ω–∞—à–∏ —Ç–æ–∫–µ–Ω—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –∑–Ω–∞—á–∏–º—ã–µ –∫—É—Å–æ—á–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å.\n",
    "- –º–æ–∂–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –¥–∞–∂–µ —Ç–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∏–∫–æ–≥–¥–∞ –Ω–µ –≤–∏–¥–µ–ª–∏!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ae7f0-2ffa-4bdb-a718-92f9632ce0bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Byte-Pair encoding tokenization (BPE)\n",
    "–û—Å–æ–±—ã–π –≤–∏–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ë—ã–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω –≤ 2015 –≥–æ–¥—É.\n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ BPE —Å–æ—Å—Ç–æ—è–ª –∏–∑ –¥–≤—É—Ö —à–∞–≥–æ–≤.\n",
    "1. –û–¥–Ω–∏–º –∏–∑ –ø—Ä–æ—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ (–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è+–ø—Ä–æ–±–µ–ª, –ø—Ä–∞–≤–∏–ª–∞) —Ä–∞–∑–±–∏–≤–∞–ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ª–æ–≤–∞. –≠—Ç–æ —Ñ–∞–∑–∞ pre-tokenization\n",
    "2. –°–æ—Å—Ç–∞–≤–ª—è–ª–∏ —á–∞—Å—Ç–æ—Å—Ç–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –≤—Å–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏—Å—å –≤ —Ç–µ–∫—Å—Ç–∞—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Å–ª–æ–≤–∞—Ä—è.\n",
    "3. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞:\n",
    "    - —Å–Ω–∞—á–∞–ª–∞ –¥–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏—Å—å\n",
    "    - –∑–∞—Ç–µ–º —Å–º–æ—Ç—Ä–∏–º –Ω–∞ —á–∞—Å—Ç–æ—Ç—É —Å–æ–≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ —Ç–µ—Ö —Ç–æ–∫–µ–Ω–æ–≤ —á—Ç–æ —É–∂–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏–∑ –Ω–∏—Ö –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã, –¥–æ–±–∞–≤–ª—è—è –∏—Ö –≤ –∏—Ç–æ–≥–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å. –ò —Ç–∞–∫ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –º—ã –Ω–∞–ø–æ–ª–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å, –ø–æ–∫–∞ –æ–±—ä–µ–º —Å–ª–æ–≤–∞—Ä—è –Ω–µ –±—É–¥–µ—Ç –Ω–∞—Å —É—Å—Ç—Ä–∞–∏–≤–∞—Ç—å.  \n",
    "\n",
    "–î–∞–Ω–Ω—ã–π –≤–∏–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ GPT-2, Roberta.\n",
    "[–ü–æ—à–∞–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞.](https://huggingface.co/course/chapter6/5?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c11f00-5e78-4204-ae1a-64b6987c0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) - –Ω–∞—à–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ—Å–ª–µ –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ee3a3-c0f1-43fd-9f93-7204cd2a7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"] # –¥–æ–±–∞–≤–∏–ª–∏ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715fd1a-a257-4d1e-9fb0-7965e28537f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ä–∞–∑–±–∏–µ–Ω–∏—è —Å–ª–æ–≤ –ø–æ –¥–≤–∞ —Å–∏–º–≤–æ–ª–∞ –∏ –∏—â–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è.\n",
    "\"hu\"  = 10 + 5 = 15\n",
    "\"un\"  = 12 + 4 = 16\n",
    "\"ug\" = 10 + 5 + 5 = 20 => vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59b179-0a17-4420-a5be-80dab35b1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–º–æ—Ç—Ä–∏–º –¥—Ä—É–≥–∏–µ —Å–æ—á–µ—Ç–∞–Ω–∏—è –±—É–∫–≤ –∏ –∑–∞–ø–æ–ª–Ω—è–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å.\n",
    "# –ö–æ–≥–¥–∞ –≤—Å–µ –¥–≤—É–±—É–∫–≤–µ–Ω–Ω—ã–µ —Å–æ—á–µ—Ç–∞–Ω–∏—è –¥–æ–±–∞–≤–∏–ª–∏, —Å–º–æ—Ç—Ä–∏–º —Ç—Ä–µ—Ö–±—É–∫–≤–µ–Ω–Ω—ã–µ –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97179d-9bc9-493c-96e3-b30625f4679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ø—É—Å—Ç—å —Å–ª–æ–≤–∞—Ä—å —É –Ω–∞—Å [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"], —Ç–æ–≥–¥–∞ —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–∑–æ–±—å—é—Ç—Å—è –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–∫–µ–Ω—ã:\n",
    "\"bug\"  = ['b', 'ug']\n",
    "\"mug\"  = ['<unk>', 'ug'] # m –Ω–µ –≤ —Å–ª–æ–≤–∞—Ä–µ, –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ —Ç–∞–∫–æ–µ –Ω–µ –≤—Å—Ç—Ä–µ—Ç–∏—Ç—å, —Ç–∞–∫ –∫–∞–∫ –∫—Ä—É–ø–Ω—ã–µ –∫–æ—Ä–ø—É—Å–∞ —Ç–µ–∫—Å—Ç–æ–≤ —Å–æ–¥–µ—Ä–∞–∂ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a70a2-a0f1-4a48-b0b7-6ef07778f903",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "–≠—Ç–æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ BERT, DistilBERT, –∏ Electra. –ê–ª–≥–æ—Ä–∏—Ç–º –≤–ø–µ—Ä–≤—ã–µ –ø–æ—è–≤–∏–ª—Å—è –≤ —Å–µ—Ç–∏ –≤ 2012 –≥–æ–¥—É –∏ –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂ –Ω–∞ BPE.  \n",
    "–ê–ª–≥–æ—Ä–∏—Ç–º —Ç–∞–∫ –∂–µ –¥–µ–ª–∞–µ—Ç –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é, –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–ª–æ–≤–∞—Ä—å –≤—Å–µ–º–∏ –≤—Å—Ç—Ä–µ—Ç–∏–≤—à–∏–º–∏—Å—è —Å–∏–º–≤–æ–ª–∞–º–∏ –∏ –∑–∞—Ç–µ–º —É—á–∏—Ç –ø—Ä–∞–≤–∏–ª–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å–∏–º–≤–æ–ª–æ–≤ –≤ –±–æ–ª–µ–µ –æ–±—â–∏–µ —Ç–æ–∫–µ–Ω—ã. –¢–æ–ª—å–∫–æ –≤ –æ—Ç–ª–∏—á–∏–µ –æ—Ç BPE, –≥–¥–µ –º—ã –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏—Å—å –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤—Å—Ç—Ä–µ—á–∞–Ω–∏–π, –∑–¥–µ—Å—å –º—ã –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–≤–º–µ—Å—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤. –¢–æ –µ—Å—Ç—å –æ–Ω —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Ç–∞–∫—É—é –≤–µ–ª–∏—á–∏–Ω—É P(A,B) = P(A,B) / P(A) * P(B)  \n",
    "[–ü–æ—à–∞–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞.](https://huggingface.co/course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac07694-7394-47a1-8d02-2fbb02da8d38",
   "metadata": {},
   "source": [
    "## Unigram\n",
    "–ú–µ—Ç–æ–¥ –ø–æ—è–≤–∏–ª—Å—è –≤ 2018 –≥–æ–¥—É. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ ALBERT, XLNet.\n",
    "–ó–¥–µ—Å—å –≤—Å–µ –¥–µ–ª–∞–µ—Ç—Å—è –Ω–∞–æ–±–æ—Ä–æ—Ç. –û–Ω —Å–Ω–∞—á–∞–ª–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–∑ –≤—Å–µ—Ö –≤—Å—Ç—Ä–µ—á–µ–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ, –∑–∞—Ç–µ–º –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤.\n",
    "–ê –ø–æ—Ç–æ–º  –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–µ–∑–∞—Ç—å —ç—Ç–æ—Ç —Å–ª–æ–≤–∞—Ä—å –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—é, –Ω–∞–ø–æ–º–∏–Ω–∞—â–∏–π WordPiece. –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ —É—á–µ—Å–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è –∫–æ–≥–¥–∞ –µ–≥–æ —Ä–∞–∑–º–µ—Ä –±—É–¥–µ—Ç —Ä–∞–≤–µ–Ω –∑–∞–¥–∞–Ω–Ω–æ–º—É –ø–∞—Ä–∞–º–µ—Ç—Ä—É. –ù–æ —Å–ª–æ–≤–∞—Ä—å –≤—Å–µ–≥–¥–∞ –±—É–¥–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã, —Ç–∞–∫ —á—Ç–æ –Ω–æ–≤—ã–µ —Å–ª–æ–≤–∞ —Ç–∞–∫ –∂–µ —É—Å–ø–µ—à–Ω–æ –±—É–¥—É—Ç —Ä–∞–∑–±–∏–≤–∞—Ç—å—Å—è –Ω–∞ —Ç–æ–∫–µ–Ω—ã.  \n",
    "[–ü–æ—à–∞–≥–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞.](https://huggingface.co/course/chapter6/7?fw=pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ede235-fafc-4ac7-b79b-9196502c77c1",
   "metadata": {},
   "source": [
    "**–ù–æ –∫–∞–∫ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤, –≥–¥–µ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤?**  \n",
    "–≠—Ç—É –ø—Ä–æ–±–ª–µ–º—É –º–æ–∂–Ω–æ —Ä–µ—à–∏—Ç—å –µ—Å–ª–∏ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∫–∞–∫ –ø—Ä–æ—Å—Ç—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ –∏ —Ç–æ—Ç –∂–µ –ø—Ä–æ–±–µ–ª —Å—á–∏—Ç–∞—Ç—å –∑–∞ —Å–∏–º–≤–æ–ª. –ó–∞—Ç–µ–º –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ç–æ—Ç –∂–µ BPE.\n",
    "\n",
    "–ù–∏–∂–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8456cb-6e52-4a32-ae08-3e2cda83c9cc",
   "metadata": {},
   "source": [
    "![comparison](./tokenizer_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd8f10-e838-4985-ad5f-ba9ef84ba1eb",
   "metadata": {},
   "source": [
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ transformers –∏ tokenizers\n",
    "–î–∞–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ —è–≤–ª—è–µ—Ç—Å—è –æ–±–µ—Ä—Ç–∫–æ–π –Ω–∞–¥ –≤—Å–µ–º–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞–º–∏. –û–Ω–∏ –ø–æ–∑–≤–æ–ª—è—é—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏ –∏ —á–∞—Å—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ (—Ç–∞–∫–∏–µ –∫–∞–∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä) –∏–∑ —Ç–æ–ø–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π.\n",
    "\n",
    "[–ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –∏—Ö —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –º–æ–∂–Ω–æ –∑–¥–µ—Å—å.](https://huggingface.co/models?sort=downloads)  \n",
    "–ü–µ—Ä–µ–π–¥—è –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å –≤—ã –º–æ–∂–µ –æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –Ω–µ–π, –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –µ–µ, –∫–∞–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–Ω–∞ –∏–º–µ–µ—Ç.\n",
    "\n",
    "**–î–∞–≤–∞–π—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É transformers –∏ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –µ–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b755324c-d613-40b2-a7d1-b618bb07a163",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (5.3b1)\n",
      "Requirement already satisfied: requests in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (1.23.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (2022.9.13)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-macosx_10_11_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: tokenizers, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.8.2 huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c6d9979-7d2f-4bd5-8f88-847222763a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a5b85823-77a4-45a5-945f-ed4127406f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7666e40c-c8e0-4a3d-986b-0afb1e22a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏–∑ –≥–æ—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ —É–∂–µ —Å–∫–∞—á–∞–Ω–∞, —Ç–æ –æ–Ω–∞ –≤–æ–∑—å–º–µ—Ç –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bb03aed9-e9cb-4b3a-b5a0-ea5346147f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–í', '##—á–µ', '##—Ä–∞', '–±—ã–ª–æ', '—Å–æ–ª', '##–Ω–µ', '##—á–Ω–æ', '.']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"–í—á–µ—Ä–∞ –±—ã–ª–æ —Å–æ–ª–Ω–µ—á–Ω–æ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8efea56d-7d4c-43b8-a5be-d65b4f5433c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'was', 'sun', '##ny', 'ye', '##ster', '##day', '.']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä —Ö—Ä–∞–Ω–∏—Ç –ø–æ–¥—Å–ª–æ–≤–∞ –∫–∞–∫ ##+—Ç–æ–∫–µ–Ω. –¢–∞–∫ –æ–Ω —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á—Ç–æ —ç—Ç–æ –Ω–µ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–π —Ç–æ–∫–µ–Ω/—Å–ª–æ–≤–æ, –∞ —Å–æ—Å—Ç–∞–≤–Ω–∞—è —á–∞—Å—Ç—å.\n",
    "tokenizer.tokenize(\"it was sunny yesterday.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eaaa00f2-0e7c-44d4-9538-d3dadab1be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4d00b988-34a6-44b5-ac72-fa6375e8b79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'was', 'sunny', 'yesterday', '.']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"it was sunny yesterday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3bd81dd1-0b45-4407-b901-cda7c3985634",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "98b8e5de-043a-4b7d-95d7-5ceb6c16920a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅit', '‚ñÅwas', '‚ñÅsunny', '‚ñÅyesterday', '.']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –≠—Ç–æ—Ç —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –±—ã–ª –æ–±—É—á–µ–Ω –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞—Ç—å –ø—Ä–æ–±–µ–ª –∫–∞–∫ –æ–±—ã—á–Ω—ã–π —Å–∏–º–≤–æ–ª. \n",
    "# –ü–æ—ç—Ç–æ–º—É –µ—Å–ª–∏ –ø–µ—Ä–µ–¥ —Å–ª–æ–≤–æ–º —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–ª—Å—è –ø—Ä–æ–±–µ–ª —Ç–æ –æ–Ω–∏ –∏ –∑–∞–ø–æ–º–Ω–∏–ª –µ–≥–æ –∫–∞–∫ –Ω–∞–±–æ—Ä —Å–∏–º–æ–≤–æ–ª–æ–≤ —Å –ø—Ä–æ–±–µ–ª–æ–º, –æ–±–æ–∑–Ω–∞—á–∏–≤ –µ–≥–æ –∫–∞–∫ _.\n",
    "# –≠—Ç–æ –∫–∞–∫ —Ä–∞–∑ –ø–æ–¥–æ–π–¥–µ—Ç –¥–ª—è –∏–µ—Ä–æ–≥–ª–∏—Ñ–æ–≤.\n",
    "tokenizer.tokenize(\"it was sunny yesterday.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76bc6e-1fa8-49fb-8dbf-c74f636bd5e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## –°—Ç—Ä–æ–∏–º —Å–≤–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
    "–ö–∞–∂–¥—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ `transformers` —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ—Å—Ç–∞–≤–ª—é—â–∏—Ö:\n",
    "1. –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä / normalization\n",
    "2. –ø—Ä–µ—Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä / pre-tokenization\n",
    "3. –º–æ–¥–µ–ª—å, –ø—Ä–∞–≤–∏–ª–∞ / model\n",
    "4. –ø–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ / post-processing\n",
    "5. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ. Decoder - –æ–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å, –∏–∑ id-–Ω–∏–∫–æ–≤ —Å–æ—Å—Ç–∞–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç \n",
    "\n",
    "![tokenization](./tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f8f26-fa47-4c77-bf50-8b0ccee22e05",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalization\n",
    "[–ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤.](https://huggingface.co/docs/tokenizers/components)  \n",
    "–ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ WordPiece, BPE, Unigram –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ç–µ–∫—Å—Ç - –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, –æ—á–∏—Å—Ç–∏—Ç—å –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –∞–∫—Ü–µ–Ω—Ç–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c420e3bc-83dd-42a6-9b5e-c1f5a9400fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b1e99359-bd55-4de8-b389-8a3d56d1d738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertNormalizer',\n",
       " 'Lowercase',\n",
       " 'NFC',\n",
       " 'NFD',\n",
       " 'NFKC',\n",
       " 'NFKD',\n",
       " 'NORMALIZERS',\n",
       " 'Nmt',\n",
       " 'Normalizer',\n",
       " 'Precompiled',\n",
       " 'Replace',\n",
       " 'Sequence',\n",
       " 'Strip',\n",
       " 'StripAccents',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'normalizers',\n",
       " 'unicode_normalizer_from_str']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(normalizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ef615395-05b3-4f45-bd53-3349be24c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import (\n",
    "    BertNormalizer, Lowercase, NFC, NFD, NFKC,\n",
    "    StripAccents, Strip, Replace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "79fd6a2c-3687-4cb7-911d-6c7f538b286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —à–∞–≥–æ–≤\n",
    "normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(\"``\", '\"'),\n",
    "    normalizers.Replace(\"''\", '\"'),\n",
    "    normalizers.Replace(\"‚Äù\", '\"'),\n",
    "    normalizers.Replace(\"‚Äú\", '\"'),\n",
    "    normalizers.Replace('Àà', \"'\"),\n",
    "    normalizers.Replace('‚Äô',\"'\"),\n",
    "    normalizers.Replace('‚Äì',\"-\"),\n",
    "    normalizers.Replace('‚Äî',\"-\"),\n",
    "    normalizers.Replace('‚àí',\"-\"),\n",
    "    normalizers.Replace('‚Ä≤',\"'\"),\n",
    "    normalizers.Replace('‚ÅÑ',\"/\"),\n",
    "    NFD(), \n",
    "    StripAccents(), \n",
    "    Lowercase(), \n",
    "    Strip()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e0937f28-6349-4c65-86c5-6eaf4bc5b6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u?'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1f375-ecc0-46d4-95ef-084282f70c8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-Tokenization\n",
    "–ü—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –º–µ–ª–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã, –∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ –ø–æ –∫–∞–∫–æ–º—É-—Ç–æ –ø—Ä–æ—Å—Ç–æ–º—É –ø—Ä–∞–≤–∏–ª—É –∏ —Ü–µ–ø–æ—á–∫–µ –ø—Ä–∞–≤–∏–ª.   \n",
    "–≠—Ç–æ—Ç —à–∞–≥ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É–∂–µ –ø—Ä–∏–º–µ—Ä–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –≤—ã—Ö–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞. –ú–æ–∂–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å —ç—Ç–æ—Ç —à–∞–≥ –∫–∞–∫ –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ç–æ–º –º–æ–≥—É—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å—Å—è –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞.\n",
    "\n",
    "[–ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–µ-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤.](https://huggingface.co/docs/tokenizers/components)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "18804c03-a2fb-4916-bfde-82a158a27416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c76bf39b-6dcf-4038-8b00-3dd2cc6c142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertPreTokenizer',\n",
       " 'ByteLevel',\n",
       " 'CharDelimiterSplit',\n",
       " 'Digits',\n",
       " 'Metaspace',\n",
       " 'PreTokenizer',\n",
       " 'Punctuation',\n",
       " 'Sequence',\n",
       " 'Split',\n",
       " 'UnicodeScripts',\n",
       " 'Whitespace',\n",
       " 'WhitespaceSplit',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'pre_tokenizers']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pre_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6e646e-d4d5-4009-bad7-2522c5c38c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpre_tokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Metaspace pre-tokenizer\n",
       "\n",
       "This pre-tokenizer replaces any whitespace by the provided replacement character.\n",
       "It then tries to split on these spaces.\n",
       "\n",
       "Args:\n",
       "    replacement (:obj:`str`, `optional`, defaults to :obj:`‚ñÅ`):\n",
       "        The replacement character. Must be exactly one character. By default we\n",
       "        use the `‚ñÅ` (U+2581) meta symbol (Same as in SentencePiece).\n",
       "\n",
       "    add_prefix_space (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether to add a space to the first word if there isn't already one. This\n",
       "        lets us treat `hello` exactly like `say hello`.\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/pre_tokenizers/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pre_tokenizers.Metaspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41deebcd-e37f-44ca-9b2f-6271fa98afb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpre_tokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDigits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindividual_digits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "This pre-tokenizer simply splits using the digits in separate tokens\n",
       "\n",
       "Args:\n",
       "    individual_digits (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        If set to True, digits will each be separated as follows::\n",
       "\n",
       "            \"Call 123 please\" -> \"Call \", \"1\", \"2\", \"3\", \" please\"\n",
       "\n",
       "        If set to False, digits will grouped as follows::\n",
       "\n",
       "            \"Call 123 please\" -> \"Call \", \"123\", \" please\"\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/pre_tokenizers/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pre_tokenizers.Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "49d7f98c-8ae3-4045-8517-3cd3f9f540f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Punctuation(),\n",
    "    pre_tokenizers.Metaspace(replacement = '_', add_prefix_space = False), \n",
    "    pre_tokenizers.Digits(individual_digits=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a7b7b03f-cc24-47d8-a749-16437e5543fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " ('!', (5, 6)),\n",
       " ('_How', (6, 10)),\n",
       " ('_are', (10, 14)),\n",
       " ('_you', (14, 18)),\n",
       " ('?', (18, 19)),\n",
       " ('_I', (19, 21)),\n",
       " (\"'\", (21, 22)),\n",
       " ('m', (22, 23)),\n",
       " ('_fine', (23, 28)),\n",
       " (',', (28, 29)),\n",
       " ('_thank', (29, 35)),\n",
       " ('_you', (35, 39)),\n",
       " ('.', (39, 40)),\n",
       " ('_', (40, 41)),\n",
       " ('1', (41, 42)),\n",
       " ('2', (42, 43)),\n",
       " ('3', (43, 44))]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you. 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b3eb1590-8b7d-4ea1-97a1-9183530b9b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', (0, 4)),\n",
       " ('_', (4, 5)),\n",
       " ('9', (5, 6)),\n",
       " ('1', (6, 7)),\n",
       " ('1', (7, 8)),\n",
       " ('!', (8, 9))]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer.pre_tokenize_str(\"Call 911!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9af870-94c7-49de-ba39-04afead76564",
   "metadata": {},
   "source": [
    "### Model\n",
    "–¢–µ–ø–µ—Ä—å –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –≤—ã–±—Ä–∞—Ç—å –∫–∞–∫—É—é –º–æ–¥–µ–ª—å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞ –º—ã —Ö–æ—Ç–∏–º —Å–¥–µ–ª–∞—Ç—å.  \n",
    "[–ó–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.](https://huggingface.co/docs/tokenizers/components)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f817fff4-b46c-47a7-befc-426eeb602570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import models as tok_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e5578bd5-1f31-43bc-b1ac-c156732d5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tok_models.BPE(unk_token=\"[UNK]\")\n",
    "# model = tok_models.Unigram()\n",
    "# model = tok_models.WordLevel()\n",
    "# model = tok_models.WordPiece(unk_token=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b624510d-0f7c-449f-977e-a2034a8ef843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtok_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "An implementation of the Unigram algorithm\n",
       "\n",
       "Args:\n",
       "    vocab (:obj:`List[Tuple[str, float]]`, `optional`):\n",
       "        A list of vocabulary items and their relative score [(\"am\", -0.2442),...]\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/models/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tok_models.Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f75d40c-6172-44ca-9347-e610b6e18940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtok_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_input_chars_per_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "An implementation of the WordPiece algorithm\n",
       "\n",
       "Args:\n",
       "    vocab (:obj:`Dict[str, int]`, `optional`):\n",
       "        A dictionnary of string keys and their ids :obj:`{\"am\": 0,...}`\n",
       "\n",
       "    unk_token (:obj:`str`, `optional`):\n",
       "        The unknown token to be used by the model.\n",
       "\n",
       "    max_input_chars_per_word (:obj:`int`, `optional`):\n",
       "        The maximum number of characters to authorize in a single word.\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/models/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tok_models.WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b0e47-6c77-417f-8660-b5d3056b6c18",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0308e17f-cfd0-4c20-bfb8-d7618a0e0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "337398a8-60fb-418a-9537-9ebf52755252",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mTemplateProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Provides a way to specify templates in order to add the special tokens to each\n",
       "input sequence as relevant.\n",
       "\n",
       "Let's take :obj:`BERT` tokenizer as an example. It uses two special tokens, used to\n",
       "delimitate each sequence. :obj:`[CLS]` is always used at the beginning of the first\n",
       "sequence, and :obj:`[SEP]` is added at the end of both the first, and the pair\n",
       "sequences. The final result looks like this:\n",
       "\n",
       "    - Single sequence: :obj:`[CLS] Hello there [SEP]`\n",
       "    - Pair sequences: :obj:`[CLS] My name is Anthony [SEP] What is my name? [SEP]`\n",
       "\n",
       "With the type ids as following::\n",
       "\n",
       "    [CLS]   ...   [SEP]   ...   [SEP]\n",
       "      0      0      0      1      1\n",
       "\n",
       "You can achieve such behavior using a TemplateProcessing::\n",
       "\n",
       "    TemplateProcessing(\n",
       "        single=\"[CLS] $0 [SEP]\",\n",
       "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
       "        special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 0)],\n",
       "    )\n",
       "\n",
       "In this example, each input sequence is identified using a ``$`` construct. This identifier\n",
       "lets us specify each input sequence, and the type_id to use. When nothing is specified,\n",
       "it uses the default values. Here are the different ways to specify it:\n",
       "\n",
       "    - Specifying the sequence, with default ``type_id == 0``: ``$A`` or ``$B``\n",
       "    - Specifying the `type_id` with default ``sequence == A``: ``$0``, ``$1``, ``$2``, ...\n",
       "    - Specifying both: ``$A:0``, ``$B:1``, ...\n",
       "\n",
       "The same construct is used for special tokens: ``<identifier>(:<type_id>)?``.\n",
       "\n",
       "**Warning**: You must ensure that you are giving the correct tokens/ids as these\n",
       "will be added to the Encoding without any further check. If the given ids correspond\n",
       "to something totally different in a `Tokenizer` using this `PostProcessor`, it\n",
       "might lead to unexpected results.\n",
       "\n",
       "Args:\n",
       "    single (:obj:`Template`):\n",
       "        The template used for single sequences\n",
       "\n",
       "    pair (:obj:`Template`):\n",
       "        The template used when both sequences are specified\n",
       "\n",
       "    special_tokens (:obj:`Tokens`):\n",
       "        The list of special tokens used in each sequences\n",
       "\n",
       "Types:\n",
       "\n",
       "    Template (:obj:`str` or :obj:`List`):\n",
       "        - If a :obj:`str` is provided, the whitespace is used as delimiter between tokens\n",
       "        - If a :obj:`List[str]` is provided, a list of tokens\n",
       "\n",
       "    Tokens (:obj:`List[Union[Tuple[int, str], Tuple[str, int], dict]]`):\n",
       "        - A :obj:`Tuple` with both a token and its associated ID, in any order\n",
       "        - A :obj:`dict` with the following keys:\n",
       "            - \"id\": :obj:`str` => The special token id, as specified in the Template\n",
       "            - \"ids\": :obj:`List[int]` => The associated IDs\n",
       "            - \"tokens\": :obj:`List[str]` => The associated tokens\n",
       "\n",
       "         The given dict expects the provided :obj:`ids` and :obj:`tokens` lists to have\n",
       "         the same length.\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/processors/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5a6fb43-3528-4ae5-8c14-ded45c2c1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc2fc3-0f30-49f0-ab54-2c642908997a",
   "metadata": {},
   "source": [
    "### Create tokenizer\n",
    "–° –ø–æ–º–æ—à—å—é —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ —Å–æ–±–∏—Ä–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a41d04ea-d0c1-4453-bc91-5ebaa952e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "77d9d078-bb79-4974-8d70-1b9eb058fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokenizer = Tokenizer(model)\n",
    "our_tokenizer.normalizer = normalizer\n",
    "our_tokenizer.pre_tokenizer = pre_tokenizer\n",
    "# our_tokenizer.post_processor = post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bceba9ee-2814-4e9a-8e9d-0c667ae0ef01",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unk token `[UNK]` not found in the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mour_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall! How are you üòÅ ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m output\u001b[38;5;241m.\u001b[39mids\n",
      "\u001b[0;31mException\u001b[0m: Unk token `[UNK]` not found in the vocabulary"
     ]
    }
   ],
   "source": [
    "output = our_tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "output.ids # –ø–æ–∫–∞ –Ω–µ –æ–±—É—á–∏–ª–∏, –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa6ce7-4afc-440b-97fb-dbc80d898175",
   "metadata": {
    "tags": []
   },
   "source": [
    "### –û–±—É—á–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –¥–∞–Ω–Ω—ã—Ö Wiki-103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "79d55b71-0ed7-4822-9956-11ca384648b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "158afc24-9c63-4429-be9f-e112c83c1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tp='train'):\n",
    "    if tp not in ['train', 'test', 'valid']:\n",
    "        raise Exception('ERROR: Wrong type of data.')\n",
    "    \n",
    "    pth = r\"wikitext-103/\" + f'wiki.{tp}.raw'\n",
    "    heading_pattern = '\\n (= ){1,}[^=]*[^=] (= ){1,}\\n \\n'\n",
    "    with open(pth, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    raw_text = re.split(heading_pattern, raw_text)\n",
    "    raw_text = [x.strip().strip('\\n').strip() for x in raw_text if x and x not in [' ', '= ']]\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "af68c23a-1643-48fe-8c06-adf86f8a7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.08 s, sys: 10.7 s, total: 17.8 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = load_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b637dd57-8c68-4a4d-83f5-9b0e5768a77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271821"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e1d2820-467c-4c0b-88ca-3b5cc3b7c87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senj≈ç no Valkyria 3 : Unrecorded Chronicles ( Japanese : Êà¶Â†¥„ÅÆ„É¥„Ç°„É´„Ç≠„É•„É™„Ç¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game \\'s opening theme was sung by May \\'n . \\n It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game \\'s expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 .'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d9e579ce-5012-4e78-9e5f-8e5d81940b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, 100000, 1000): # –ù–µ –±—É–¥–µ–º –Ω–∞ –≤—Å–µ–º –æ–±—ä–µ–º–µ\n",
    "        yield train_data[i : i + 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4c48b8f6-fba6-4916-8e6b-14793a9b4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEC_TOKENS = [\"[UNK]\", \"[PAD]\", \"[MASK]\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0e807c77-32d7-494d-a866-4e1eadcf70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "        vocab_size=100000, \n",
    "        min_frequency=0,\n",
    "        show_progress=True,\n",
    "        special_tokens=SPEC_TOKENS, \n",
    "        continuing_subword_prefix='##'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "522135fb-a1e2-4ed2-b73e-380dd4eae12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 48s, sys: 33.7 s, total: 5min 21s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "our_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19d4b3-0b6c-4a1c-be67-c14ab344f673",
   "metadata": {},
   "source": [
    "### –ú–µ—Ç–æ–¥—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd196f-1106-44da-84c1-9cefcaa5ce94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "our_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "575ebbf6-3982-4cba-8f64-f51c0aee4d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "faf381f4-ec33-4a29-a117-1d3f58362198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Œµ'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.id_to_token(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b0888c56-040c-4207-86d9-30e277ff9a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5793"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.token_to_id('_how')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "65b15d37-323d-41d3-8279-71fb5209d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokenizer.token_to_id('bla') # –¢–∞–∫–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "21d7efb5-5f47-47b2-998c-e0d627fe20e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mour_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Add the given tokens to the vocabulary\n",
       "\n",
       "The given tokens are added only if they don't already exist in the vocabulary.\n",
       "Each token then gets a new attributed id.\n",
       "\n",
       "Args:\n",
       "    tokens (A :obj:`List` of :class:`~tokenizers.AddedToken` or :obj:`str`):\n",
       "        The list of tokens we want to add to the vocabulary. Each token can be either a\n",
       "        string or an instance of :class:`~tokenizers.AddedToken` for more customization.\n",
       "\n",
       "Returns:\n",
       "    :obj:`int`: The number of tokens that were created in the vocabulary\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?our_tokenizer.add_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e1dce-9597-4bb6-a037-b09b7eb7a8d7",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d0c01249-f03c-4ca2-a6a5-42824c239bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = our_tokenizer.encode(\"H√©ll√≤ h√¥w are √º?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "96a1860e-1b01-4e43-8ad9-211bf82f1a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', '##ello', '_how', '_are', '_u', '?']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "59f5a5bb-644b-43e5-9190-1b847fe44fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 13770, 5793, 5546, 5436, 34]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8b34c930-adbe-4106-a296-90093c235ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 5)\n",
      "Token: ##ello\n",
      "Original string part: √©ll√≤\n"
     ]
    }
   ],
   "source": [
    "# –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å –∫–∞–∫–æ–π –Ω–∞ –∫–∞–∫—É—é –ø–æ–∑–∏—Ü—É—é –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è i-—ã–π —Ç–æ–∫–µ–Ω\n",
    "pos = encode.token_to_chars(1)\n",
    "print('Position:', pos)\n",
    "print('Token:', encode.tokens[1])\n",
    "print('Original string part:', \"H√©ll√≤ h√¥w are √º?\"[pos[0]: pos[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d1785665-9ba6-4e91-ace1-8bc4c22e5318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–º–µ—Ä (–Ω–µ ID!!!) —Ç–æ–∫–µ–Ω–∞ –ø–æ –ø–æ–∑–∏—Ü–∏–∏ —Å–∏–º–≤–æ–ª–∞ –≤ —Å—Ç—Ä–æ–∫–µ\n",
    "encode.char_to_token(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4efdc530-60d1-4f62-ae82-e69222076fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##ello'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.id_to_token(encode.ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf954fc-74aa-45f7-b6ae-1a0ca44b7b1f",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "–û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å. –°–æ–±–∏—Ä–∞–µ–º –∏–∑ id-—à–Ω–∏–∫–æ–≤ –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "372d7a41-7ca0-4a2d-97be-7293a6276991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "365dd334-febb-4c97-bb4d-1a77146543ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BPEDecoder',\n",
       " 'ByteLevel',\n",
       " 'CTC',\n",
       " 'Decoder',\n",
       " 'Metaspace',\n",
       " 'Sequence',\n",
       " 'WordPiece',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'decoders']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "71e7dabc-35c0-45ab-a259-f07080acd40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h ##ello _how _are _u ?'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.decode(encode.ids) # –î–µ—Ñ–æ–ª—Ç–Ω—ã–π –¥–µ–∫–æ–¥–µ—Ä –≤—ã–≥–ª—è–¥–∏—Ç –Ω–µ —Å—É–ø–µ—Ä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2d06a110-c8db-48a2-8c82-8e3c6530c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoders.Sequence([\n",
    "    # decoders.BPEDecoder(suffix='##'),\n",
    "    decoders.WordPiece(prefix='##'),\n",
    "    decoders.Metaspace(replacement='_'),  \n",
    "])\n",
    "our_tokenizer.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3b0bfedf-44c9-46d7-b1a0-f42fb7d2500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello  how  are  u?'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.decode(encode.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc33d9-eb1e-40fc-980d-29d25118db16",
   "metadata": {},
   "source": [
    "### Save/Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5fde850f-9dae-40f1-a610-b57324157140",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokenizer.save(\"Tokenizer_BPE100k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a7c5665f-dd92-4fbb-9466-f15e9acc168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_our_tokenizer = Tokenizer.from_file(\"Tokenizer_BPE100k.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa176542-5f8d-4d84-9805-e9569380f1d2",
   "metadata": {},
   "source": [
    "### Fast tokenizing\n",
    "–ú—ã –º–æ–∂–µ–º –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—à —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –æ–±–µ—Ä–Ω—É—Ç—å –≤ –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—É—é –æ–±–µ—Ä—Ç–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "00f2750f-5958-41c9-b639-95539388fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d56add73-0f35-4002-a570-d91612e3c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_our_tokenizer_fast = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"Tokenizer_BPE100k.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be070a85-0a7e-4ff0-a035-527bb742c234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
