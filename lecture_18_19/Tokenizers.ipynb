{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ad1bded-53f7-45e1-ae46-830e39fee916",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "Как вы помните, токенизация это процесс когда мы разбиваем текст на слова или части слов. Каждый токенайзер имеет внутри себя какую то таблицу, где на против id-шника стоит токен (слово или часть). И в процессе токенизации он разбивает текст на слова из своего словаря и затем меняет слово на его id. В итоге после токенизации мы получаем не текст а список id-шников.\n",
    "\n",
    "Вопрос только как составить эту таблицу-словарь и как разбить текст на составляющие. Тем более что не во всех языках есть понятие пробела, да разбивать по пробелам и пунктуации не самый точный способ.\n",
    "\n",
    "Мы уже знакомы с токенайзером от SpaCy и он был rule-based токенизатор, то есть основан на правилах. Так же мы использовали токенизаторы основанные на пробелах и пунктуации.\n",
    "\n",
    "Какие минусы могут быть при разбиении текста по пробелам и пунктуации:  \n",
    "- словарь может стать очень большим, так как слов очень много  \n",
    "- сложно написать корректные правила разделения, где-то точка знак пунктуации, где часть смайла, где-то часть троеточия и таких нюансов тысячи\n",
    "    \n",
    "    \n",
    "Сегодня углубимся в логику построения токенизаторов, посмотрим какие типы токенизаторов бывают и поймем как и где можно найти готовые токенизаторы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7431261-57c1-404d-973c-3111225c681f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word tokenization\n",
    "Если взять большой набор текстов, то вы получите огромное количество уникальных слов. Если вы хотите все их запомнить и обрабатвывать, то вам придется держать огромную таблицу внутри токенайзера, что скажется на скорости/памяти. Но даже не это страшно, вы будете копить в словаре все опечатки, а так же слова которые встречались раз или два. По таким редким словам у вас нет статистики и любая модель, которая будет подстраиваться под них будет учить явно не то что вы хотите от нее."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90dcf8-57dd-4fa2-a30b-3dbae78e1a26",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Character tokenization\n",
    "Проблему можно решить перейдя на символьный токенайзер. Новые символы появляются значительно реже чем слова.  \n",
    "Но этот подход рождает свои проблемы:  \n",
    "- текст становится очень длительной последовательностью, моделям уже очень сложно уловить зависимости в этой последовательности  \n",
    "- эмбединги символов несут очень мало полезной информации для нас, в отличии от эмбедингов слов. \n",
    "\n",
    "**Токенайзеры не содеражат эмбединги! Они нужны для последующего использования в моделях.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758a1b0-3db9-4b3f-84c1-a14e96f531d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Subword tokenization\n",
    "Данный вид токенайзеров разбивает текст на токены, где токен может быть словом или частью слова.   \n",
    "Принцип следующий - Если слово часто встречается, то мы запоминаем его целиков как отдельный токен. Если редко, то смотрим какие части слова встречаются наиболее часто и разбиваем его на них.  \n",
    "Получается что решаем следующие проблемы:\n",
    "- ушли от проблемы безконтрольного разрастания словаря\n",
    "- наши токены представляют собой значимые кусочки информации, которые можно потом интерпретировать.\n",
    "- можем обрабатывать даже те слова, которые никогда не видели!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10ae7f0-2ffa-4bdb-a718-92f9632ce0bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Byte-Pair encoding tokenization (BPE)\n",
    "Особый вид токенизатора. Был предложен в 2015 году.\n",
    "Алгоритм обучение токенизатора BPE состоял из двух шагов.\n",
    "1. Одним из простых токенизаторов (пунктуация+пробел, правила) разбивали текст на слова. Это фаза pre-tokenization\n",
    "2. Составляли частостную таблицу всех токенов, которые встретились в текстах при обучении словаря.\n",
    "3. Формируем итоговый словарь токенизатора:\n",
    "    - сначала добавляем в словарь все символы, которые встретились\n",
    "    - затем смотрим на частоту совстречаемости тех токенов что уже есть в словаре и формируем из них новые токены, добавляя их в итоговый словарь. И так рекурсивно мы наполняем словарь, пока объем словаря не будет нас устраивать.  \n",
    "\n",
    "Данный вид токенизаторов используется в GPT-2, Roberta.\n",
    "[Пошаговая реализация алгоритма.](https://huggingface.co/course/chapter6/5?fw=pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c11f00-5e78-4204-ae1a-64b6987c0c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример\n",
    "(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) - наша таблица после претокенизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1ee3a3-c0f1-43fd-9f93-7204cd2a7dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"] # добавили все символы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715fd1a-a257-4d1e-9fb0-7965e28537f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пробуем разные разбиения слов по два символа и ищем самые часто встречающиеся.\n",
    "\"hu\"  = 10 + 5 = 15\n",
    "\"un\"  = 12 + 4 = 16\n",
    "\"ug\" = 10 + 5 + 5 = 20 => vocabulary = [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59b179-0a17-4420-a5be-80dab35b1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим другие сочетания букв и заполняем в словарь.\n",
    "# Когда все двубуквенные сочетания добавили, смотрим трехбуквенные и так далее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97179d-9bc9-493c-96e3-b30625f4679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пусть словарь у нас [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"], тогда следующие примеры разобьются на соответствующие токены:\n",
    "\"bug\"  = ['b', 'ug']\n",
    "\"mug\"  = ['<unk>', 'ug'] # m не в словаре, на практике такое не встретить, так как крупные корпуса текстов содераж все возможные символы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a70a2-a0f1-4a48-b0b7-6ef07778f903",
   "metadata": {},
   "source": [
    "## WordPiece\n",
    "Этот алгоритм токенизации используется в BERT, DistilBERT, и Electra. Алгоритм впервые появился в сети в 2012 году и очень похож на BPE.  \n",
    "Алгоритм так же делает претокенизацию, инициализирует словарь всеми встретившимися символами и затем учит правила объединения символов в более общие токены. Только в отличие от BPE, где мы ориентировались на количество совстречаний, здесь мы максимизируем вероятность совместного использования токенов. То есть он смотрит на такую величину P(A,B) = P(A,B) / P(A) * P(B)  \n",
    "[Пошаговая реализация алгоритма.](https://huggingface.co/course/chapter6/6?fw=pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac07694-7394-47a1-8d02-2fbb02da8d38",
   "metadata": {},
   "source": [
    "## Unigram\n",
    "Метод появился в 2018 году. Используется в ALBERT, XLNet.\n",
    "Здесь все делается наоборот. Он сначала составляет огромный словарь из всех встреченных токенов в тексте, затем подстроки токенов.\n",
    "А потом  начинает резать этот словарь по критерию, напоминащий WordPiece. Процедура учесения словаря останавливается когда его размер будет равен заданному параметру. Но словарь всегда будет содержать все символы, так что новые слова так же успешно будут разбиваться на токены.  \n",
    "[Пошаговая реализация алгоритма.](https://huggingface.co/course/chapter6/7?fw=pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ede235-fafc-4ac7-b79b-9196502c77c1",
   "metadata": {},
   "source": [
    "**Но как токенизировать текст состоящий из иероглифов, где нет пробелов?**  \n",
    "Эту проблему можно решить если рассматривать текст как простую последовательность символов и тот же пробел считать за символ. Затем можно применить тот же BPE.\n",
    "\n",
    "Ниже изображено сравнение алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8456cb-6e52-4a32-ae08-3e2cda83c9cc",
   "metadata": {},
   "source": [
    "![comparison](./tokenizer_comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd8f10-e838-4985-ad5f-ba9ef84ba1eb",
   "metadata": {},
   "source": [
    "# Библиотеки transformers и tokenizers\n",
    "Данная библиотеки является оберткой над всеми современными разработками. Они позволяют использовать модели и части пайплайна (такие как токенизатор) из топовых статей.\n",
    "\n",
    "[Посмотреть какие модели на данный момент находятся в их хранилище можно здесь.](https://huggingface.co/models?sort=downloads)  \n",
    "Перейдя в конкретную модель вы може ознакомиться с ней, посмотреть как установить ее, какие возможности она имеет.\n",
    "\n",
    "**Давайте установим библиотеку transformers и воспользуемся ее встроенными средствами для токенизации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b755324c-d613-40b2-a7d1-b618bb07a163",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (5.3b1)\n",
      "Requirement already satisfied: requests in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (1.23.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.8.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from transformers) (2022.9.13)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-macosx_10_11_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Installing collected packages: tokenizers, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.8.2 huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c6d9979-7d2f-4bd5-8f88-847222763a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a5b85823-77a4-45a5-945f-ed4127406f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer # Импортируем токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7666e40c-c8e0-4a3d-986b-0afb1e22a845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим токенизатор из готовой модели, если она уже скачана, то она возьмет из локального хранилища\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "bb03aed9-e9cb-4b3a-b5a0-ea5346147f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['В', '##че', '##ра', 'было', 'сол', '##не', '##чно', '.']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Вчера было солнечно.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8efea56d-7d4c-43b8-a5be-d65b4f5433c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'was', 'sun', '##ny', 'ye', '##ster', '##day', '.']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Токенайзер хранит подслова как ##+токен. Так он сигнализирует что это не независимый токен/слово, а составная часть.\n",
    "tokenizer.tokenize(\"it was sunny yesterday.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eaaa00f2-0e7c-44d4-9538-d3dadab1be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4d00b988-34a6-44b5-ac72-fa6375e8b79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'was', 'sunny', 'yesterday', '.']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"it was sunny yesterday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3bd81dd1-0b45-4407-b901-cda7c3985634",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "98b8e5de-043a-4b7d-95d7-5ceb6c16920a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁it', '▁was', '▁sunny', '▁yesterday', '.']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Этот токенайзер был обучен воспринимать пробел как обычный символ. \n",
    "# Поэтому если перед словом часто встречался пробел то они и запомнил его как набор симоволов с пробелом, обозначив его как _.\n",
    "# Это как раз подойдет для иероглифов.\n",
    "tokenizer.tokenize(\"it was sunny yesterday.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c76bc6e-1fa8-49fb-8dbf-c74f636bd5e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Строим свой токенизатор\n",
    "Каждый токенизатор в библиотеке `transformers` состоит из следующих составлющих:\n",
    "1. нормализатор / normalization\n",
    "2. претокенайзер / pre-tokenization\n",
    "3. модель, правила / model\n",
    "4. пост-обработка / post-processing\n",
    "5. Опционально. Decoder - обратный процесс, из id-ников составить исходный текст \n",
    "\n",
    "![tokenization](./tokenization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f8f26-fa47-4c77-bf50-8b0ccee22e05",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalization\n",
    "[Здесь можно посмотреть описание нормализаторов.](https://huggingface.co/docs/tokenizers/components)  \n",
    "Перед тем как использовать алгоритмы токенизации WordPiece, BPE, Unigram необходимо подготовить текст - привести к нижнему регистру, очистить от лишних символов и акцентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c420e3bc-83dd-42a6-9b5e-c1f5a9400fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b1e99359-bd55-4de8-b389-8a3d56d1d738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertNormalizer',\n",
       " 'Lowercase',\n",
       " 'NFC',\n",
       " 'NFD',\n",
       " 'NFKC',\n",
       " 'NFKD',\n",
       " 'NORMALIZERS',\n",
       " 'Nmt',\n",
       " 'Normalizer',\n",
       " 'Precompiled',\n",
       " 'Replace',\n",
       " 'Sequence',\n",
       " 'Strip',\n",
       " 'StripAccents',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'normalizers',\n",
       " 'unicode_normalizer_from_str']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(normalizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ef615395-05b3-4f45-bd53-3349be24c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.normalizers import (\n",
    "    BertNormalizer, Lowercase, NFC, NFD, NFKC,\n",
    "    StripAccents, Strip, Replace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "79fd6a2c-3687-4cb7-911d-6c7f538b286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем нормализатор из последовательности шагов\n",
    "normalizer = normalizers.Sequence([\n",
    "    normalizers.Replace(\"``\", '\"'),\n",
    "    normalizers.Replace(\"''\", '\"'),\n",
    "    normalizers.Replace(\"”\", '\"'),\n",
    "    normalizers.Replace(\"“\", '\"'),\n",
    "    normalizers.Replace('ˈ', \"'\"),\n",
    "    normalizers.Replace('’',\"'\"),\n",
    "    normalizers.Replace('–',\"-\"),\n",
    "    normalizers.Replace('—',\"-\"),\n",
    "    normalizers.Replace('−',\"-\"),\n",
    "    normalizers.Replace('′',\"'\"),\n",
    "    normalizers.Replace('⁄',\"/\"),\n",
    "    NFD(), \n",
    "    StripAccents(), \n",
    "    Lowercase(), \n",
    "    Strip()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e0937f28-6349-4c65-86c5-6eaf4bc5b6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello how are u?'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.normalize_str(\"Héllò hôw are ü?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d1f375-ecc0-46d4-95ef-084282f70c8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pre-Tokenization\n",
    "Претокенизация заключается в разделении текста на мелкие объекты, как правило по какому-то простому правилу и цепочке правил.   \n",
    "Этот шаг позволяет уже примерно оценить выход токенизатора. Можно рассматривать этот шаг как деление на слова, которые потом могут разделиться на подслова.\n",
    "\n",
    "[Здесь можно посмотреть описание пре-токенизаторов.](https://huggingface.co/docs/tokenizers/components)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "18804c03-a2fb-4916-bfde-82a158a27416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import pre_tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c76bf39b-6dcf-4038-8b00-3dd2cc6c142e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BertPreTokenizer',\n",
       " 'ByteLevel',\n",
       " 'CharDelimiterSplit',\n",
       " 'Digits',\n",
       " 'Metaspace',\n",
       " 'PreTokenizer',\n",
       " 'Punctuation',\n",
       " 'Sequence',\n",
       " 'Split',\n",
       " 'UnicodeScripts',\n",
       " 'Whitespace',\n",
       " 'WhitespaceSplit',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'pre_tokenizers']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(pre_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6e646e-d4d5-4009-bad7-2522c5c38c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpre_tokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_prefix_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Metaspace pre-tokenizer\n",
       "\n",
       "This pre-tokenizer replaces any whitespace by the provided replacement character.\n",
       "It then tries to split on these spaces.\n",
       "\n",
       "Args:\n",
       "    replacement (:obj:`str`, `optional`, defaults to :obj:`▁`):\n",
       "        The replacement character. Must be exactly one character. By default we\n",
       "        use the `▁` (U+2581) meta symbol (Same as in SentencePiece).\n",
       "\n",
       "    add_prefix_space (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
       "        Whether to add a space to the first word if there isn't already one. This\n",
       "        lets us treat `hello` exactly like `say hello`.\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/pre_tokenizers/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pre_tokenizers.Metaspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41deebcd-e37f-44ca-9b2f-6271fa98afb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mpre_tokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDigits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindividual_digits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "This pre-tokenizer simply splits using the digits in separate tokens\n",
       "\n",
       "Args:\n",
       "    individual_digits (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
       "        If set to True, digits will each be separated as follows::\n",
       "\n",
       "            \"Call 123 please\" -> \"Call \", \"1\", \"2\", \"3\", \" please\"\n",
       "\n",
       "        If set to False, digits will grouped as follows::\n",
       "\n",
       "            \"Call 123 please\" -> \"Call \", \"123\", \" please\"\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/pre_tokenizers/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?pre_tokenizers.Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "49d7f98c-8ae3-4045-8517-3cd3f9f540f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Punctuation(),\n",
    "    pre_tokenizers.Metaspace(replacement = '_', add_prefix_space = False), \n",
    "    pre_tokenizers.Digits(individual_digits=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a7b7b03f-cc24-47d8-a749-16437e5543fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " ('!', (5, 6)),\n",
       " ('_How', (6, 10)),\n",
       " ('_are', (10, 14)),\n",
       " ('_you', (14, 18)),\n",
       " ('?', (18, 19)),\n",
       " ('_I', (19, 21)),\n",
       " (\"'\", (21, 22)),\n",
       " ('m', (22, 23)),\n",
       " ('_fine', (23, 28)),\n",
       " (',', (28, 29)),\n",
       " ('_thank', (29, 35)),\n",
       " ('_you', (35, 39)),\n",
       " ('.', (39, 40)),\n",
       " ('_', (40, 41)),\n",
       " ('1', (41, 42)),\n",
       " ('2', (42, 43)),\n",
       " ('3', (43, 44))]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you. 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b3eb1590-8b7d-4ea1-97a1-9183530b9b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Call', (0, 4)),\n",
       " ('_', (4, 5)),\n",
       " ('9', (5, 6)),\n",
       " ('1', (6, 7)),\n",
       " ('1', (7, 8)),\n",
       " ('!', (8, 9))]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokenizer.pre_tokenize_str(\"Call 911!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9af870-94c7-49de-ba39-04afead76564",
   "metadata": {},
   "source": [
    "### Model\n",
    "Теперь нам необходимо выбрать какую модель токенайзера мы хотим сделать.  \n",
    "[Здесь можно посмотреть описание доступных моделей.](https://huggingface.co/docs/tokenizers/components)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f817fff4-b46c-47a7-befc-426eeb602570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import models as tok_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e5578bd5-1f31-43bc-b1ac-c156732d5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tok_models.BPE(unk_token=\"[UNK]\")\n",
    "# model = tok_models.Unigram()\n",
    "# model = tok_models.WordLevel()\n",
    "# model = tok_models.WordPiece(unk_token=\"[UNK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b624510d-0f7c-449f-977e-a2034a8ef843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtok_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "An implementation of the Unigram algorithm\n",
       "\n",
       "Args:\n",
       "    vocab (:obj:`List[Tuple[str, float]]`, `optional`):\n",
       "        A list of vocabulary items and their relative score [(\"am\", -0.2442),...]\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/models/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tok_models.Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f75d40c-6172-44ca-9347-e610b6e18940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mtok_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_input_chars_per_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "An implementation of the WordPiece algorithm\n",
       "\n",
       "Args:\n",
       "    vocab (:obj:`Dict[str, int]`, `optional`):\n",
       "        A dictionnary of string keys and their ids :obj:`{\"am\": 0,...}`\n",
       "\n",
       "    unk_token (:obj:`str`, `optional`):\n",
       "        The unknown token to be used by the model.\n",
       "\n",
       "    max_input_chars_per_word (:obj:`int`, `optional`):\n",
       "        The maximum number of characters to authorize in a single word.\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/models/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?tok_models.WordPiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225b0e47-6c77-417f-8660-b5d3056b6c18",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0308e17f-cfd0-4c20-bfb8-d7618a0e0a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "337398a8-60fb-418a-9537-9ebf52755252",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mTemplateProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Provides a way to specify templates in order to add the special tokens to each\n",
       "input sequence as relevant.\n",
       "\n",
       "Let's take :obj:`BERT` tokenizer as an example. It uses two special tokens, used to\n",
       "delimitate each sequence. :obj:`[CLS]` is always used at the beginning of the first\n",
       "sequence, and :obj:`[SEP]` is added at the end of both the first, and the pair\n",
       "sequences. The final result looks like this:\n",
       "\n",
       "    - Single sequence: :obj:`[CLS] Hello there [SEP]`\n",
       "    - Pair sequences: :obj:`[CLS] My name is Anthony [SEP] What is my name? [SEP]`\n",
       "\n",
       "With the type ids as following::\n",
       "\n",
       "    [CLS]   ...   [SEP]   ...   [SEP]\n",
       "      0      0      0      1      1\n",
       "\n",
       "You can achieve such behavior using a TemplateProcessing::\n",
       "\n",
       "    TemplateProcessing(\n",
       "        single=\"[CLS] $0 [SEP]\",\n",
       "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
       "        special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 0)],\n",
       "    )\n",
       "\n",
       "In this example, each input sequence is identified using a ``$`` construct. This identifier\n",
       "lets us specify each input sequence, and the type_id to use. When nothing is specified,\n",
       "it uses the default values. Here are the different ways to specify it:\n",
       "\n",
       "    - Specifying the sequence, with default ``type_id == 0``: ``$A`` or ``$B``\n",
       "    - Specifying the `type_id` with default ``sequence == A``: ``$0``, ``$1``, ``$2``, ...\n",
       "    - Specifying both: ``$A:0``, ``$B:1``, ...\n",
       "\n",
       "The same construct is used for special tokens: ``<identifier>(:<type_id>)?``.\n",
       "\n",
       "**Warning**: You must ensure that you are giving the correct tokens/ids as these\n",
       "will be added to the Encoding without any further check. If the given ids correspond\n",
       "to something totally different in a `Tokenizer` using this `PostProcessor`, it\n",
       "might lead to unexpected results.\n",
       "\n",
       "Args:\n",
       "    single (:obj:`Template`):\n",
       "        The template used for single sequences\n",
       "\n",
       "    pair (:obj:`Template`):\n",
       "        The template used when both sequences are specified\n",
       "\n",
       "    special_tokens (:obj:`Tokens`):\n",
       "        The list of special tokens used in each sequences\n",
       "\n",
       "Types:\n",
       "\n",
       "    Template (:obj:`str` or :obj:`List`):\n",
       "        - If a :obj:`str` is provided, the whitespace is used as delimiter between tokens\n",
       "        - If a :obj:`List[str]` is provided, a list of tokens\n",
       "\n",
       "    Tokens (:obj:`List[Union[Tuple[int, str], Tuple[str, int], dict]]`):\n",
       "        - A :obj:`Tuple` with both a token and its associated ID, in any order\n",
       "        - A :obj:`dict` with the following keys:\n",
       "            - \"id\": :obj:`str` => The special token id, as specified in the Template\n",
       "            - \"ids\": :obj:`List[int]` => The associated IDs\n",
       "            - \"tokens\": :obj:`List[str]` => The associated tokens\n",
       "\n",
       "         The given dict expects the provided :obj:`ids` and :obj:`tokens` lists to have\n",
       "         the same length.\n",
       "\u001b[0;31mFile:\u001b[0m           /Users/u14510182/Documents/python_for_nlp_stud/venv/lib/python3.9/site-packages/tokenizers/processors/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?TemplateProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5a6fb43-3528-4ae5-8c14-ded45c2c1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecc2fc3-0f30-49f0-ab54-2c642908997a",
   "metadata": {},
   "source": [
    "### Create tokenizer\n",
    "С помошью созданных этапов собираем итоговый токенизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a41d04ea-d0c1-4453-bc91-5ebaa952e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "77d9d078-bb79-4974-8d70-1b9eb058fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokenizer = Tokenizer(model)\n",
    "our_tokenizer.normalizer = normalizer\n",
    "our_tokenizer.pre_tokenizer = pre_tokenizer\n",
    "# our_tokenizer.post_processor = post_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bceba9ee-2814-4e9a-8e9d-0c667ae0ef01",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unk token `[UNK]` not found in the vocabulary",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mour_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, y\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall! How are you 😁 ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m output\u001b[38;5;241m.\u001b[39mids\n",
      "\u001b[0;31mException\u001b[0m: Unk token `[UNK]` not found in the vocabulary"
     ]
    }
   ],
   "source": [
    "output = our_tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
    "output.ids # пока не обучили, ничего не работает."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa6ce7-4afc-440b-97fb-dbc80d898175",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Обучаем токенизатор на данных Wiki-103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "79d55b71-0ed7-4822-9956-11ca384648b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "158afc24-9c63-4429-be9f-e112c83c1597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tp='train'):\n",
    "    if tp not in ['train', 'test', 'valid']:\n",
    "        raise Exception('ERROR: Wrong type of data.')\n",
    "    \n",
    "    pth = r\"wikitext-103/\" + f'wiki.{tp}.raw'\n",
    "    heading_pattern = '\\n (= ){1,}[^=]*[^=] (= ){1,}\\n \\n'\n",
    "    with open(pth, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    raw_text = re.split(heading_pattern, raw_text)\n",
    "    raw_text = [x.strip().strip('\\n').strip() for x in raw_text if x and x not in [' ', '= ']]\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "af68c23a-1643-48fe-8c06-adf86f8a7ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.08 s, sys: 10.7 s, total: 17.8 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = load_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b637dd57-8c68-4a4d-83f5-9b0e5768a77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271821"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2e1d2820-467c-4c0b-88ca-3b5cc3b7c87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game \\'s opening theme was sung by May \\'n . \\n It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game \\'s expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 .'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d9e579ce-5012-4e78-9e5f-8e5d81940b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, 100000, 1000): # Не будем на всем объеме\n",
    "        yield train_data[i : i + 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4c48b8f6-fba6-4916-8e6b-14793a9b4686",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEC_TOKENS = [\"[UNK]\", \"[PAD]\", \"[MASK]\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0e807c77-32d7-494d-a866-4e1eadcf70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(\n",
    "        vocab_size=100000, \n",
    "        min_frequency=0,\n",
    "        show_progress=True,\n",
    "        special_tokens=SPEC_TOKENS, \n",
    "        continuing_subword_prefix='##'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "522135fb-a1e2-4ed2-b73e-380dd4eae12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 4min 48s, sys: 33.7 s, total: 5min 21s\n",
      "Wall time: 38.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "our_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19d4b3-0b6c-4a1c-be67-c14ab344f673",
   "metadata": {},
   "source": [
    "### Методы токенизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd196f-1106-44da-84c1-9cefcaa5ce94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "our_tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "575ebbf6-3982-4cba-8f64-f51c0aee4d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "faf381f4-ec33-4a29-a117-1d3f58362198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ε'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.id_to_token(234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b0888c56-040c-4207-86d9-30e277ff9a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5793"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.token_to_id('_how')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "65b15d37-323d-41d3-8279-71fb5209d638",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokenizer.token_to_id('bla') # Такого токена не существует"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "21d7efb5-5f47-47b2-998c-e0d627fe20e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mour_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Add the given tokens to the vocabulary\n",
       "\n",
       "The given tokens are added only if they don't already exist in the vocabulary.\n",
       "Each token then gets a new attributed id.\n",
       "\n",
       "Args:\n",
       "    tokens (A :obj:`List` of :class:`~tokenizers.AddedToken` or :obj:`str`):\n",
       "        The list of tokens we want to add to the vocabulary. Each token can be either a\n",
       "        string or an instance of :class:`~tokenizers.AddedToken` for more customization.\n",
       "\n",
       "Returns:\n",
       "    :obj:`int`: The number of tokens that were created in the vocabulary\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?our_tokenizer.add_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e1dce-9597-4bb6-a037-b09b7eb7a8d7",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d0c01249-f03c-4ca2-a6a5-42824c239bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = our_tokenizer.encode(\"Héllò hôw are ü?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "96a1860e-1b01-4e43-8ad9-211bf82f1a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h', '##ello', '_how', '_are', '_u', '?']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "59f5a5bb-644b-43e5-9190-1b847fe44fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 13770, 5793, 5546, 5436, 34]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "8b34c930-adbe-4106-a296-90093c235ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: (1, 5)\n",
      "Token: ##ello\n",
      "Original string part: éllò\n"
     ]
    }
   ],
   "source": [
    "# Показывает с какой на какую позицую в исходной строке приходится i-ый токен\n",
    "pos = encode.token_to_chars(1)\n",
    "print('Position:', pos)\n",
    "print('Token:', encode.tokens[1])\n",
    "print('Original string part:', \"Héllò hôw are ü?\"[pos[0]: pos[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d1785665-9ba6-4e91-ace1-8bc4c22e5318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Возвращает номер (не ID!!!) токена по позиции символа в строке\n",
    "encode.char_to_token(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4efdc530-60d1-4f62-ae82-e69222076fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##ello'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.id_to_token(encode.ids[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf954fc-74aa-45f7-b6ae-1a0ca44b7b1f",
   "metadata": {},
   "source": [
    "### Decoding\n",
    "Обратный процесс. Собираем из id-шников исходный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "372d7a41-7ca0-4a2d-97be-7293a6276991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "365dd334-febb-4c97-bb4d-1a77146543ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BPEDecoder',\n",
       " 'ByteLevel',\n",
       " 'CTC',\n",
       " 'Decoder',\n",
       " 'Metaspace',\n",
       " 'Sequence',\n",
       " 'WordPiece',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'decoders']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(decoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "71e7dabc-35c0-45ab-a259-f07080acd40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h ##ello _how _are _u ?'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.decode(encode.ids) # Дефолтный декодер выглядит не супер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2d06a110-c8db-48a2-8c82-8e3c6530c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoders.Sequence([\n",
    "    # decoders.BPEDecoder(suffix='##'),\n",
    "    decoders.WordPiece(prefix='##'),\n",
    "    decoders.Metaspace(replacement='_'),  \n",
    "])\n",
    "our_tokenizer.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3b0bfedf-44c9-46d7-b1a0-f42fb7d2500c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello  how  are  u?'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokenizer.decode(encode.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbc33d9-eb1e-40fc-980d-29d25118db16",
   "metadata": {},
   "source": [
    "### Save/Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5fde850f-9dae-40f1-a610-b57324157140",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokenizer.save(\"Tokenizer_BPE100k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a7c5665f-dd92-4fbb-9466-f15e9acc168b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_our_tokenizer = Tokenizer.from_file(\"Tokenizer_BPE100k.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa176542-5f8d-4d84-9805-e9569380f1d2",
   "metadata": {},
   "source": [
    "### Fast tokenizing\n",
    "Мы можем зафиксировать наш токенайзер и обернуть в более быструю обертку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "00f2750f-5958-41c9-b639-95539388fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d56add73-0f35-4002-a570-d91612e3c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_our_tokenizer_fast = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"Tokenizer_BPE100k.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be070a85-0a7e-4ff0-a035-527bb742c234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
